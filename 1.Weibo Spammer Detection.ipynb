{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "During our analyse, we found that many weibo messages are spams. So before performing any formal analyses, it is necessary to detect and filter out spams. This is the purpose of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bao\\Google Drive\\weibo & twitter\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('week36.csv')\n",
    "csv_f = csv.reader(f)\n",
    "\n",
    "data = []\n",
    "for row in csv_f:\n",
    "    data.append(row)\n",
    "data = data[1:]  # delete the header "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mdOTjWMh6o', '', 'uWYNHUVBU', '', '\\xe7\\x9a\\xae\\xe7\\x9a\\xae\\xe6\\x97\\xb6\\xe5\\x85\\x89\\xe6\\x9c\\xba', '1', ' \\xe6\\x80\\xa7\\xe6\\x84\\x9f\\xe8\\x95\\xbe\\xe4\\xb8\\x9d\\xe9\\x95\\x82\\xe7\\xa9\\xba\\xe9\\xb1\\xbc\\xe5\\x98\\xb4\\xe5\\x87\\x89\\xe9\\x9e\\x8b\\xef\\xbc\\x8c\\xe7\\xbd\\x97\\xe9\\xa9\\xac\\xe9\\xa3\\x8e\\xe5\\x8d\\x81\\xe8\\xb6\\xb3\\xe5\\x95\\x8a\\xef\\xbc\\x81\\xe3\\x80\\x8b\\xe3\\x80\\x8b\\xe3\\x80\\x8b\\xe3\\x80\\x8bhttp://t.cn/zWrRwoz', '', '2012-09-03 00:00:20', '', '']\n"
     ]
    }
   ],
   "source": [
    "print data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#user id\n",
    "userids = []\n",
    "for line in data:\n",
    "    userid = line[2]\n",
    "    userids.append(userid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uWYNHUVBU', 'uRLOCRBNC', 'u5KY5H0QX', 'u5KY5H0QX', 'uY02ICHJO', 'uZXYRCKCP', 'uWWBE4HMZ', 'uWWBCGJE4', 'uZDQRIOWI', 'uZD34OXSS']\n",
      "3263536\n"
     ]
    }
   ],
   "source": [
    "print userids[:10]\n",
    "print len(userids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144078\n"
     ]
    }
   ],
   "source": [
    "# unique user id\n",
    "userids_set = set(userids)\n",
    "print len(userids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time \n",
    "times = []\n",
    "for line in data:\n",
    "    time = line[8]\n",
    "    times.append(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263536\n",
      "['2012-09-03 00:00:20', '2012-09-03 00:00:00', '2012-09-03 00:00:12', '2012-09-03 00:00:10', '2012-09-03 21:40:47', '2012-09-03 00:00:11', '2012-09-03 00:00:07', '2012-09-03 00:03:19', '2012-09-03 00:00:12', '2012-09-03 00:00:14']\n"
     ]
    }
   ],
   "source": [
    "print len(times)\n",
    "print times[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# texts\n",
    "texts = []\n",
    "for line in data:\n",
    "    text = line[6]\n",
    "    try:\n",
    "        text_chn = text.decode('utf8')\n",
    "    except:\n",
    "        pass\n",
    "    texts.append(text_chn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'\\u3010\\u7f8e\\u56fe\\u5206\\u4eab\\u3011'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of @ in each message\n",
    "mention_count = []\n",
    "for text in texts:\n",
    "    mention_count.append(text.count('@'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of hatshtag in each message\n",
    "hashtag_count = []\n",
    "for text in texts:\n",
    "    hashtag_count.append(text.count('#')/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of url in each message\n",
    "url_count = []\n",
    "for text in texts:\n",
    "    url_count.append(text.count('http://'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spammers and normal users\n",
    "I manually labeled 700 spammers and normal users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n",
      "['u02HIMXNX', 'u02HRIHXV', 'u02HRLGB1', 'u02HRUQGI', 'u02HRVTBP']\n"
     ]
    }
   ],
   "source": [
    "f3 = open('spammer list.csv')\n",
    "csv_f3 = csv.reader(f3)\n",
    "\n",
    "spammer = []\n",
    "for row in csv_f3:\n",
    "    spammer.append(row[0])\n",
    "print len(spammer)\n",
    "print spammer[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n",
      "['u02LNWVSW', 'u02LNX4ET', 'u02LNXC2Y', 'u02LNXKZK', 'u02LNXMQU']\n"
     ]
    }
   ],
   "source": [
    "f4 = open('normal user list.csv')\n",
    "csv_f4 = csv.reader(f4)\n",
    "\n",
    "normal = []\n",
    "for row in csv_f4:\n",
    "    normal.append(row[0])\n",
    "print len(normal)\n",
    "print normal[:5]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n"
     ]
    }
   ],
   "source": [
    "spammer_set = set(spammer)\n",
    "normal_set = set(normal)\n",
    "print len(spammer_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "3263536\n"
     ]
    }
   ],
   "source": [
    "spammer_b = []\n",
    "for user in userids:\n",
    "    if user in spammer_set:\n",
    "        spammer_b.append(1)\n",
    "    elif user in normal_set:\n",
    "        spammer_b.append(0)\n",
    "    else:\n",
    "        spammer_b.append(2)  # 2 means unkown\n",
    "print spammer_b[:100]\n",
    "print len(spammer_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrct a dataframe: Weibo as the unit analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'userid': userids, 'text': texts, 'is.spammer': spammer_b, 'url_count': url_count, \n",
    "                   'mention_count': mention_count, 'hashtag_count': hashtag_count, 'time': times})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>is.spammer</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>url_count</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>性感蕾丝镂空鱼嘴凉鞋，罗马风十足啊！》》》》http://t.cn/zWrRwoz</td>\n",
       "      <td>2012-09-03 00:00:20</td>\n",
       "      <td>1</td>\n",
       "      <td>uWYNHUVBU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>【美图分享】</td>\n",
       "      <td>2012-09-03 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>uRLOCRBNC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>佛经说：“人生有八万四千种烦恼。”小人物有小人物的烦恼，大人物有大人物的悲辛，都是不能避免。...</td>\n",
       "      <td>2012-09-03 00:00:12</td>\n",
       "      <td>0</td>\n",
       "      <td>u5KY5H0QX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>年轻没有借口</td>\n",
       "      <td>2012-09-03 00:00:10</td>\n",
       "      <td>0</td>\n",
       "      <td>u5KY5H0QX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>用自己照片作桌面会不会太……[睡觉]</td>\n",
       "      <td>2012-09-03 21:40:47</td>\n",
       "      <td>0</td>\n",
       "      <td>uY02ICHJO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hashtag_count  is.spammer  mention_count  \\\n",
       "0              0           2              0   \n",
       "1              0           2              0   \n",
       "2              0           2              1   \n",
       "3              0           2              0   \n",
       "4              0           2              0   \n",
       "\n",
       "                                                text                 time  \\\n",
       "0          性感蕾丝镂空鱼嘴凉鞋，罗马风十足啊！》》》》http://t.cn/zWrRwoz  2012-09-03 00:00:20   \n",
       "1                                             【美图分享】  2012-09-03 00:00:00   \n",
       "2  佛经说：“人生有八万四千种烦恼。”小人物有小人物的烦恼，大人物有大人物的悲辛，都是不能避免。...  2012-09-03 00:00:12   \n",
       "3                                             年轻没有借口  2012-09-03 00:00:10   \n",
       "4                                 用自己照片作桌面会不会太……[睡觉]  2012-09-03 21:40:47   \n",
       "\n",
       "   url_count     userid  \n",
       "0          1  uWYNHUVBU  \n",
       "1          0  uRLOCRBNC  \n",
       "2          0  u5KY5H0QX  \n",
       "3          0  u5KY5H0QX  \n",
       "4          0  uY02ICHJO  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.to_csv('alldata', encoding='utf-8', index=False)\n",
    "#df10 = pd.read_csv('alldata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059503\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>is_oversea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u1O52TEBM</td>\n",
       "      <td>傻子 过去就过去了 越想只会让自己 越来越TM看不起自己 恨自己不是吗？有什么了不起的 人不...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u0AGHHPXK</td>\n",
       "      <td>闲来无事，想破人QQ空间密码，于是点击了一个美女的空间，提问：“请问男生最喜欢什么花！”我暗...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u0VP213WJ</td>\n",
       "      <td>Google Map真是一项伟大的发明，一个人在语言不通的地方一样可以满世界＂流浪＂，对我这...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u0VPP3KI2</td>\n",
       "      <td>@uK3RQFVG5： 来围观一下这个@彼得潘定制手工甜品</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u1CIX3EO2</td>\n",
       "      <td>太可爱了[哈哈]//@uZQXIDN4T：  [哈哈]//@ukn：  谁都有二的岁月</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userid                                               text  \\\n",
       "0  u1O52TEBM  傻子 过去就过去了 越想只会让自己 越来越TM看不起自己 恨自己不是吗？有什么了不起的 人不...   \n",
       "1  u0AGHHPXK  闲来无事，想破人QQ空间密码，于是点击了一个美女的空间，提问：“请问男生最喜欢什么花！”我暗...   \n",
       "2  u0VP213WJ  Google Map真是一项伟大的发明，一个人在语言不通的地方一样可以满世界＂流浪＂，对我这...   \n",
       "3  u0VPP3KI2                      @uK3RQFVG5： 来围观一下这个@彼得潘定制手工甜品   \n",
       "4  u1CIX3EO2        太可爱了[哈哈]//@uZQXIDN4T：  [哈哈]//@ukn：  谁都有二的岁月   \n",
       "\n",
       "   hashtag_count  mention_count  url_count  is_oversea  \n",
       "0              0              1          0           1  \n",
       "1              0              0          0           0  \n",
       "2              0              0          0           0  \n",
       "3              0              2          0           1  \n",
       "4              0              2          0           0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add time to 'normal_all' file\n",
    "df10 = pd.read_csv('normal_all')\n",
    "print len(df10)\n",
    "df10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84560\n"
     ]
    }
   ],
   "source": [
    "complete_normal_list = df10['userid'].tolist()\n",
    "complete_normal_list = set(complete_normal_list)\n",
    "print len(complete_normal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>is.spammer</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>url_count</th>\n",
       "      <th>userid</th>\n",
       "      <th>is_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>性感蕾丝镂空鱼嘴凉鞋，罗马风十足啊！》》》》http://t.cn/zWrRwoz</td>\n",
       "      <td>2012-09-03 00:00:20</td>\n",
       "      <td>1</td>\n",
       "      <td>uWYNHUVBU</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>【美图分享】</td>\n",
       "      <td>2012-09-03 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>uRLOCRBNC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>佛经说：“人生有八万四千种烦恼。”小人物有小人物的烦恼，大人物有大人物的悲辛，都是不能避免。...</td>\n",
       "      <td>2012-09-03 00:00:12</td>\n",
       "      <td>0</td>\n",
       "      <td>u5KY5H0QX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>年轻没有借口</td>\n",
       "      <td>2012-09-03 00:00:10</td>\n",
       "      <td>0</td>\n",
       "      <td>u5KY5H0QX</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>用自己照片作桌面会不会太……[睡觉]</td>\n",
       "      <td>2012-09-03 21:40:47</td>\n",
       "      <td>0</td>\n",
       "      <td>uY02ICHJO</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hashtag_count  is.spammer  mention_count  \\\n",
       "0              0           2              0   \n",
       "1              0           2              0   \n",
       "2              0           2              1   \n",
       "3              0           2              0   \n",
       "4              0           2              0   \n",
       "\n",
       "                                                text                 time  \\\n",
       "0          性感蕾丝镂空鱼嘴凉鞋，罗马风十足啊！》》》》http://t.cn/zWrRwoz  2012-09-03 00:00:20   \n",
       "1                                             【美图分享】  2012-09-03 00:00:00   \n",
       "2  佛经说：“人生有八万四千种烦恼。”小人物有小人物的烦恼，大人物有大人物的悲辛，都是不能避免。...  2012-09-03 00:00:12   \n",
       "3                                             年轻没有借口  2012-09-03 00:00:10   \n",
       "4                                 用自己照片作桌面会不会太……[睡觉]  2012-09-03 21:40:47   \n",
       "\n",
       "   url_count     userid  is_normal  \n",
       "0          1  uWYNHUVBU          0  \n",
       "1          0  uRLOCRBNC          0  \n",
       "2          0  u5KY5H0QX          0  \n",
       "3          0  u5KY5H0QX          0  \n",
       "4          0  uY02ICHJO          0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_normal = []\n",
    "for index, row in df.iterrows():\n",
    "    if row['userid'] in complete_normal_list:\n",
    "        is_normal.append(1)\n",
    "    else:\n",
    "        is_normal.append(0)\n",
    "df['is_normal'] = is_normal\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263536\n",
      "892539\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>is.spammer</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>url_count</th>\n",
       "      <th>userid</th>\n",
       "      <th>is_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>冷言冷语。。。继续。。。这样很好。。。</td>\n",
       "      <td>2012-09-03 00:03:19</td>\n",
       "      <td>0</td>\n",
       "      <td>uWWBCGJE4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>傻子 过去就过去了 越想只会让自己 越来越TM看不起自己 恨自己不是吗？有什么了不起的 人不...</td>\n",
       "      <td>2012-09-03 00:04:12</td>\n",
       "      <td>0</td>\n",
       "      <td>u1O52TEBM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>闲来无事，想破人QQ空间密码，于是点击了一个美女的空间，提问：“请问男生最喜欢什么花！”我暗...</td>\n",
       "      <td>2012-09-03 00:02:10</td>\n",
       "      <td>0</td>\n",
       "      <td>u0AGHHPXK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>北京今天，泛起秋意。</td>\n",
       "      <td>2012-09-03 00:07:36</td>\n",
       "      <td>0</td>\n",
       "      <td>uVGJLC4S5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>@_@</td>\n",
       "      <td>2012-09-03 02:09:32</td>\n",
       "      <td>0</td>\n",
       "      <td>uATZPTH53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hashtag_count  is.spammer  mention_count  \\\n",
       "7               0           2              0   \n",
       "10              0           2              1   \n",
       "14              0           2              0   \n",
       "21              0           2              0   \n",
       "22              0           2              2   \n",
       "\n",
       "                                                 text                 time  \\\n",
       "7                                 冷言冷语。。。继续。。。这样很好。。。  2012-09-03 00:03:19   \n",
       "10  傻子 过去就过去了 越想只会让自己 越来越TM看不起自己 恨自己不是吗？有什么了不起的 人不...  2012-09-03 00:04:12   \n",
       "14  闲来无事，想破人QQ空间密码，于是点击了一个美女的空间，提问：“请问男生最喜欢什么花！”我暗...  2012-09-03 00:02:10   \n",
       "21                                         北京今天，泛起秋意。  2012-09-03 00:07:36   \n",
       "22                                                @_@  2012-09-03 02:09:32   \n",
       "\n",
       "    url_count     userid  is_normal  \n",
       "7           0  uWWBCGJE4          1  \n",
       "10          0  u1O52TEBM          1  \n",
       "14          0  u0AGHHPXK          1  \n",
       "21          0  uVGJLC4S5          1  \n",
       "22          0  uATZPTH53          1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df11 = df[df['is_normal'] == 1]\n",
    "print len(df)\n",
    "print len(df11)\n",
    "df11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>url_count</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>冷言冷语。。。继续。。。这样很好。。。</td>\n",
       "      <td>2012-09-03 00:03:19</td>\n",
       "      <td>0</td>\n",
       "      <td>uWWBCGJE4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>傻子 过去就过去了 越想只会让自己 越来越TM看不起自己 恨自己不是吗？有什么了不起的 人不...</td>\n",
       "      <td>2012-09-03 00:04:12</td>\n",
       "      <td>0</td>\n",
       "      <td>u1O52TEBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>闲来无事，想破人QQ空间密码，于是点击了一个美女的空间，提问：“请问男生最喜欢什么花！”我暗...</td>\n",
       "      <td>2012-09-03 00:02:10</td>\n",
       "      <td>0</td>\n",
       "      <td>u0AGHHPXK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>北京今天，泛起秋意。</td>\n",
       "      <td>2012-09-03 00:07:36</td>\n",
       "      <td>0</td>\n",
       "      <td>uVGJLC4S5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>@_@</td>\n",
       "      <td>2012-09-03 02:09:32</td>\n",
       "      <td>0</td>\n",
       "      <td>uATZPTH53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hashtag_count  mention_count  \\\n",
       "7               0              0   \n",
       "10              0              1   \n",
       "14              0              0   \n",
       "21              0              0   \n",
       "22              0              2   \n",
       "\n",
       "                                                 text                 time  \\\n",
       "7                                 冷言冷语。。。继续。。。这样很好。。。  2012-09-03 00:03:19   \n",
       "10  傻子 过去就过去了 越想只会让自己 越来越TM看不起自己 恨自己不是吗？有什么了不起的 人不...  2012-09-03 00:04:12   \n",
       "14  闲来无事，想破人QQ空间密码，于是点击了一个美女的空间，提问：“请问男生最喜欢什么花！”我暗...  2012-09-03 00:02:10   \n",
       "21                                         北京今天，泛起秋意。  2012-09-03 00:07:36   \n",
       "22                                                @_@  2012-09-03 02:09:32   \n",
       "\n",
       "    url_count     userid  \n",
       "7           0  uWWBCGJE4  \n",
       "10          0  u1O52TEBM  \n",
       "14          0  u0AGHHPXK  \n",
       "21          0  uVGJLC4S5  \n",
       "22          0  uATZPTH53  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df11=df11.drop('is.spammer', axis = 1)\n",
    "df11=df11.drop('is_normal', axis = 1)\n",
    "df11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df11.to_csv('normal_all2', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User as the unit analysis (Aggregate data based on the same user id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_user = pd.DataFrame()\n",
    "df_user['weibo_count'] = df.groupby(['userid']).size()\n",
    "df_user['is.spammer'] = df.groupby(['userid'])['is.spammer'].mean()\n",
    "df_user['url_mean'] = df.groupby(['userid'])['url_count'].mean()\n",
    "df_user['mention_mean'] = df.groupby(['userid'])['mention_count'].mean()\n",
    "df_user['hashtag_mean'] = df.groupby(['userid'])['hashtag_count'].mean()\n",
    "df_user['alltext'] = df.groupby(['userid'])['text'].agg(lambda x: ''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weibo_count</th>\n",
       "      <th>is.spammer</th>\n",
       "      <th>url_mean</th>\n",
       "      <th>mention_mean</th>\n",
       "      <th>hashtag_mean</th>\n",
       "      <th>alltext</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>u02HI3WUO</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>那些最好的朋友，看起来很正常。但只有我知道：他们是神经病。[偷笑]你若不离不弃，我必生死相依...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HI5DC5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>身体是革命的本钱！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HIC5JL</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>看起来都很猛!去过几次也没见过如此美景呢,#随手拍#“毛爷爷”说：过了黄洋界，险处不须看。但...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HICEX0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>爪，我要看//@ukn：  看的时候 就在想 总菊咋就让过了呢能感动你的，是因为和你相似</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HICM0L</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1.346154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>敲山震虎吧~//@uDG02DDM3：  总算给了我一次力顶新华社的机会！久旱逢甘雨//@u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           weibo_count  is.spammer  url_mean  mention_mean  hashtag_mean  \\\n",
       "userid                                                                     \n",
       "u02HI3WUO            6           0  0.000000      0.000000      0.000000   \n",
       "u02HI5DC5            1           2  0.000000      0.000000      0.000000   \n",
       "u02HIC5JL           90           0  0.055556      1.444444      0.044444   \n",
       "u02HICEX0            2           2  0.000000      0.500000      0.000000   \n",
       "u02HICM0L           26           0  0.076923      1.346154      0.000000   \n",
       "\n",
       "                                                     alltext  \n",
       "userid                                                        \n",
       "u02HI3WUO  那些最好的朋友，看起来很正常。但只有我知道：他们是神经病。[偷笑]你若不离不弃，我必生死相依...  \n",
       "u02HI5DC5                                          身体是革命的本钱！  \n",
       "u02HIC5JL  看起来都很猛!去过几次也没见过如此美景呢,#随手拍#“毛爷爷”说：过了黄洋界，险处不须看。但...  \n",
       "u02HICEX0       爪，我要看//@ukn：  看的时候 就在想 总菊咋就让过了呢能感动你的，是因为和你相似  \n",
       "u02HICM0L  敲山震虎吧~//@uDG02DDM3：  总算给了我一次力顶新华社的机会！久旱逢甘雨//@u...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那些最好的朋友，看起来很正常。但只有我知道：他们是神经病。[偷笑]你若不离不弃，我必生死相依。有些话，你不经意的说出口，我却很认真的难过。自己好，才是真的好。香港兰博基尼手机新品发布会！有很多人都说:\"秀恩爱死得快!\"以前我不信,现在我信了.哈哈哈!不许笑!不可以幸灾乐祸!其实我想说:\"这取决于你爱没爱对人,你对一个重名重利的人付出真心?你认为他会用真心回报你吗?也许他会,但是那人绝对不是你.第一你长得并不美若天仙;第二你并不家财万贯.劝你先把前两者拥有再来爱这样的人吧!\"\n"
     ]
    }
   ],
   "source": [
    "print df_user['alltext'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use SVM classifers for spammer detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train a classifer based on the labeled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weibo_count</th>\n",
       "      <th>is.spammer</th>\n",
       "      <th>url_mean</th>\n",
       "      <th>mention_mean</th>\n",
       "      <th>hashtag_mean</th>\n",
       "      <th>alltext</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>u02HI3WUO</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>那些最好的朋友，看起来很正常。但只有我知道：他们是神经病。[偷笑]你若不离不弃，我必生死相依...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HIC5JL</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>看起来都很猛!去过几次也没见过如此美景呢,#随手拍#“毛爷爷”说：过了黄洋界，险处不须看。但...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HICM0L</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1.346154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>敲山震虎吧~//@uDG02DDM3：  总算给了我一次力顶新华社的机会！久旱逢甘雨//@u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HIFZWV</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>转发微博[哈哈] //@u5KYQF03K： //@uOQZ3UZCE：  转发微博同意  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HIGB0T</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>转 //@uP2ZKNHEX： 无转 岂有此理！ //@uP2ZKNHEX： 无老任有进步，...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           weibo_count  is.spammer  url_mean  mention_mean  hashtag_mean  \\\n",
       "userid                                                                     \n",
       "u02HI3WUO            6           0  0.000000      0.000000      0.000000   \n",
       "u02HIC5JL           90           0  0.055556      1.444444      0.044444   \n",
       "u02HICM0L           26           0  0.076923      1.346154      0.000000   \n",
       "u02HIFZWV            9           0  0.000000      2.333333      0.000000   \n",
       "u02HIGB0T            8           0  0.000000      2.000000      0.000000   \n",
       "\n",
       "                                                     alltext  \n",
       "userid                                                        \n",
       "u02HI3WUO  那些最好的朋友，看起来很正常。但只有我知道：他们是神经病。[偷笑]你若不离不弃，我必生死相依...  \n",
       "u02HIC5JL  看起来都很猛!去过几次也没见过如此美景呢,#随手拍#“毛爷爷”说：过了黄洋界，险处不须看。但...  \n",
       "u02HICM0L  敲山震虎吧~//@uDG02DDM3：  总算给了我一次力顶新华社的机会！久旱逢甘雨//@u...  \n",
       "u02HIFZWV  转发微博[哈哈] //@u5KYQF03K： //@uOQZ3UZCE：  转发微博同意  ...  \n",
       "u02HIGB0T  转 //@uP2ZKNHEX： 无转 岂有此理！ //@uP2ZKNHEX： 无老任有进步，...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lab0 = df_user.loc[(df_user['is.spammer'] == 0) | (df_user['is.spammer'] == 1) ]\n",
    "print len(df_lab0)\n",
    "df_lab0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import cross_validation\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "y = df_lab0['is.spammer'].tolist()\n",
    "print y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features\n",
    "Five features: weibo count, if there is url in the message, if there is hatshtag, if there is @, and texts (BoW). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features: weibo count, hashtag, url, and @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 90]\n",
      "[0.0, 0.055555555555555552]\n",
      "[0.0, 1.4444444444444444]\n",
      "[0.0, 0.044444444444444446]\n"
     ]
    }
   ],
   "source": [
    "weibo = df_lab0['weibo_count'].tolist()\n",
    "url = df_lab0['url_mean'].tolist()\n",
    "mention = df_lab0['mention_mean'].tolist()\n",
    "hashtag = df_lab0['hashtag_mean'].tolist()\n",
    "print weibo[:2]\n",
    "print url[:2]\n",
    "print mention[:2]\n",
    "print hashtag[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = df_lab0['alltext'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text2 = []\n",
    "for t in text:\n",
    "    text2.append(t.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if '。'.decode('utf-8') in text[0]:\n",
    "    print \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\xe3\\x80\\x82'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove Chinese punctuation\n",
    "punc = ['。', '，', '！', '？', '：', '“', '”', '（', '）', '【', '】', '、', '％', '.', ':', '(', ')', '[', ']', '《', '》', '', '；', '～', '＂'] \n",
    "punc = punc+['#', ',', '!', '?', '+', '=', '-', '~', '\"', '<', '>', '/', '^', ';']\n",
    "#punc = [p.decode('utf-8') for p in punc]\n",
    "text_rem_punc = []\n",
    "for t in text2:\n",
    "    for c in punc:\n",
    "        if c in t:\n",
    "            t = t.replace(c, '')\n",
    "    text_rem_punc.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "敲山震虎吧~//@uDG02DDM3：  总算给了我一次力顶新华社的机会！久旱逢甘雨//@uDSKY12K：  //@uJWAARF3V：  话语轻松，让人沉重。//@uZXYUQ0DK：  转发微博。主要欧洲没有第一个敢吃螃蟹的人~//@uUPCA5NKO：  别忘了姜丝！//@uEMB1WHCK：  还有黄酒//@uUPWF0CG0：  //@uR1WHCMFP：  [话筒]//@uDSKY12K：  //@uZ32TP1UP：  回复@uBK14TKEP： 谢谢，转发。 //@uBK14TKEP： 有2号出席会议的消息http://t.cn/zWdHHtn曾几何时我也是一个船人~外国客人就不能用国产车接待？一定要奔驰？换了车他们不做？他们强制要求？他们每天都来？转发微博//@uDSKY12K：  //@uONPBWOO3：  [怒] 质问政府，质问执法者，到底是人违法，还是物违法呢？！你是制止违法行为呢？！还是毁坏私人财产呢？！ //@ukn： [怒]//@uP2ZKNHEX：  真气派转发微博转发微博”中国高储蓄为经济提供充分资金保障“怎么感觉这么别扭呢~//@uWWBBHERW：  转发微博分享图片 我在:http://t.cn/zWDOiaW《潘币往事》//@uDG02DDM3： ≪潘石屹十万个为什么？≫励志故事啊~//@uK3RZZQRS：  [心]//@uMLLC0VKT：  //@uPKFWV0GB：  [心] //@u0YUMTBXF： 【伟大母爱：用我鲜血换儿子一次生命的机会】[心]哽咽根本原因是想起了自己仰望别人屁股的童年了吧 //@uSIAVDOGY：  让他把从银行贷的款都还清了，估计他就不会是＂首善＂了。 //@uI43Y5XIC：  //@假装在纽约：你们就当我是中国的脑残粉好了。我就是曾经在街上看城管打小贩哭了的人，我就是每次看见吃不饱饭的孩子和有十几块名表的书记就眼红哽咽的人，我就是半夜看网上强拆视频，中国人保护自己房子，看得嚎啕大哭的人。你们不用怀疑，我的祖国每天都在让它的人民感动 //@uDSKY12K：  转发微博你每条微博都要@ukn： 她一定请你吃饭~//@uHRVOCY34：  //@ukn： 不要造谣，不是中国。//@ukn：  //@ukn：  吴仪老师已弹会《渔舟唱晚》等乐曲了，这对大家是极好的鼓励！学古筝没有年龄手型等限制，我们都可以向吴仪老师一样，通过学习古筝陶冶心境，修身养性，享受有品位、有追求、有境界的人生！[赞][心]@被封存的记忆wy转发微博//@uYC3WXAXA：  //@ukn：  //@uBK1P5HZK： 现在当个兵精送礼也得三万五万的。\n",
      "敲山震虎吧@uDG02DDM3  总算给了我一次力顶新华社的机会久旱逢甘雨@uDSKY12K  @uJWAARF3V  话语轻松让人沉重@uZXYUQ0DK  转发微博主要欧洲没有第一个敢吃螃蟹的人@uUPCA5NKO  别忘了姜丝@uEMB1WHCK  还有黄酒@uUPWF0CG0  @uR1WHCMFP  话筒@uDSKY12K  @uZ32TP1UP  回复@uBK14TKEP 谢谢转发 @uBK14TKEP 有2号出席会议的消息httptcnzWdHHtn曾几何时我也是一个船人外国客人就不能用国产车接待一定要奔驰换了车他们不做他们强制要求他们每天都来转发微博@uDSKY12K  @uONPBWOO3  怒 质问政府质问执法者到底是人违法还是物违法呢你是制止违法行为呢还是毁坏私人财产呢 @ukn 怒@uP2ZKNHEX  真气派转发微博转发微博中国高储蓄为经济提供充分资金保障怎么感觉这么别扭呢@uWWBBHERW  转发微博分享图片 我在httptcnzWDOiaW潘币往事@uDG02DDM3 ≪潘石屹十万个为什么≫励志故事啊@uK3RZZQRS  心@uMLLC0VKT  @uPKFWV0GB  心 @u0YUMTBXF 伟大母爱用我鲜血换儿子一次生命的机会心哽咽根本原因是想起了自己仰望别人屁股的童年了吧 @uSIAVDOGY  让他把从银行贷的款都还清了估计他就不会是首善了 @uI43Y5XIC  @假装在纽约你们就当我是中国的脑残粉好了我就是曾经在街上看城管打小贩哭了的人我就是每次看见吃不饱饭的孩子和有十几块名表的书记就眼红哽咽的人我就是半夜看网上强拆视频中国人保护自己房子看得嚎啕大哭的人你们不用怀疑我的祖国每天都在让它的人民感动 @uDSKY12K  转发微博你每条微博都要@ukn 她一定请你吃饭@uHRVOCY34  @ukn 不要造谣不是中国@ukn  @ukn  吴仪老师已弹会渔舟唱晚等乐曲了这对大家是极好的鼓励学古筝没有年龄手型等限制我们都可以向吴仪老师一样通过学习古筝陶冶心境修身养性享受有品位有追求有境界的人生赞心@被封存的记忆wy转发微博@uYC3WXAXA  @ukn  @uBK1P5HZK 现在当个兵精送礼也得三万五万的\n"
     ]
    }
   ],
   "source": [
    "print text2[2]\n",
    "print text_rem_punc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba  # for Chinese text segementation\n",
    "seg_list = []\n",
    "for i in range(len(text_rem_punc)):\n",
    "    words = jieba.cut(text_rem_punc[i], cut_all=False)\n",
    "    seg_list.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n",
      "<generator object cut at 0x00000000CFF08630>\n"
     ]
    }
   ],
   "source": [
    "print len(seg_list)\n",
    "print seg_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seg_list2 = []\n",
    "for i in range(len(seg_list)):\n",
    "    words = [w.encode('utf-8') for w in seg_list[i]]\n",
    "    seg_list2.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\xe9\\x82\\xa3\\xe4\\xba\\x9b', '\\xe6\\x9c\\x80\\xe5\\xa5\\xbd', '\\xe7\\x9a\\x84', '\\xe6\\x9c\\x8b\\xe5\\x8f\\x8b', '\\xe7\\x9c\\x8b\\xe8\\xb5\\xb7\\xe6\\x9d\\xa5', '\\xe5\\xbe\\x88', '\\xe6\\xad\\xa3\\xe5\\xb8\\xb8', '\\xe4\\xbd\\x86', '\\xe5\\x8f\\xaa\\xe6\\x9c\\x89', '\\xe6\\x88\\x91', '\\xe7\\x9f\\xa5\\xe9\\x81\\x93', '\\xe4\\xbb\\x96\\xe4\\xbb\\xac', '\\xe6\\x98\\xaf', '\\xe7\\xa5\\x9e\\xe7\\xbb\\x8f\\xe7\\x97\\x85', '\\xe5\\x81\\xb7\\xe7\\xac\\x91', '\\xe4\\xbd\\xa0', '\\xe8\\x8b\\xa5', '\\xe4\\xb8\\x8d\\xe7\\xa6\\xbb', '\\xe4\\xb8\\x8d\\xe5\\xbc\\x83', '\\xe6\\x88\\x91', '\\xe5\\xbf\\x85', '\\xe7\\x94\\x9f\\xe6\\xad\\xbb\\xe7\\x9b\\xb8\\xe4\\xbe\\x9d', '\\xe6\\x9c\\x89\\xe4\\xba\\x9b', '\\xe8\\xaf\\x9d', '\\xe4\\xbd\\xa0', '\\xe4\\xb8\\x8d\\xe7\\xbb\\x8f\\xe6\\x84\\x8f', '\\xe7\\x9a\\x84', '\\xe8\\xaf\\xb4', '\\xe5\\x87\\xba\\xe5\\x8f\\xa3', '\\xe6\\x88\\x91', '\\xe5\\x8d\\xb4', '\\xe5\\xbe\\x88', '\\xe8\\xae\\xa4\\xe7\\x9c\\x9f', '\\xe7\\x9a\\x84', '\\xe9\\x9a\\xbe\\xe8\\xbf\\x87', '\\xe8\\x87\\xaa\\xe5\\xb7\\xb1', '\\xe5\\xa5\\xbd', '\\xe6\\x89\\x8d', '\\xe6\\x98\\xaf', '\\xe7\\x9c\\x9f\\xe7\\x9a\\x84', '\\xe5\\xa5\\xbd', '\\xe9\\xa6\\x99\\xe6\\xb8\\xaf', '\\xe5\\x85\\xb0\\xe5\\x8d\\x9a\\xe5\\x9f\\xba\\xe5\\xb0\\xbc', '\\xe6\\x89\\x8b\\xe6\\x9c\\xba', '\\xe6\\x96\\xb0\\xe5\\x93\\x81', '\\xe5\\x8f\\x91\\xe5\\xb8\\x83\\xe4\\xbc\\x9a', '\\xe6\\x9c\\x89', '\\xe5\\xbe\\x88\\xe5\\xa4\\x9a', '\\xe4\\xba\\xba', '\\xe9\\x83\\xbd', '\\xe8\\xaf\\xb4\\xe7\\xa7\\x80', '\\xe6\\x81\\xa9\\xe7\\x88\\xb1', '\\xe6\\xad\\xbb', '\\xe5\\xbe\\x97', '\\xe5\\xbf\\xab', '\\xe4\\xbb\\xa5\\xe5\\x89\\x8d', '\\xe6\\x88\\x91', '\\xe4\\xb8\\x8d\\xe4\\xbf\\xa1', '\\xe7\\x8e\\xb0\\xe5\\x9c\\xa8', '\\xe6\\x88\\x91\\xe4\\xbf\\xa1', '\\xe4\\xba\\x86', '\\xe5\\x93\\x88\\xe5\\x93\\x88\\xe5\\x93\\x88', '\\xe4\\xb8\\x8d\\xe8\\xae\\xb8', '\\xe7\\xac\\x91', '\\xe4\\xb8\\x8d', '\\xe5\\x8f\\xaf\\xe4\\xbb\\xa5', '\\xe5\\xb9\\xb8\\xe7\\x81\\xbe\\xe4\\xb9\\x90\\xe7\\xa5\\xb8', '\\xe5\\x85\\xb6\\xe5\\xae\\x9e', '\\xe6\\x88\\x91', '\\xe6\\x83\\xb3', '\\xe8\\xaf\\xb4', '\\xe8\\xbf\\x99', '\\xe5\\x8f\\x96\\xe5\\x86\\xb3\\xe4\\xba\\x8e', '\\xe4\\xbd\\xa0', '\\xe7\\x88\\xb1', '\\xe6\\xb2\\xa1', '\\xe7\\x88\\xb1', '\\xe5\\xaf\\xb9', '\\xe4\\xba\\xba', '\\xe4\\xbd\\xa0', '\\xe5\\xaf\\xb9', '\\xe4\\xb8\\x80\\xe4\\xb8\\xaa', '\\xe9\\x87\\x8d\\xe5\\x90\\x8d', '\\xe9\\x87\\x8d\\xe5\\x88\\xa9', '\\xe7\\x9a\\x84', '\\xe4\\xba\\xba', '\\xe4\\xbb\\x98\\xe5\\x87\\xba', '\\xe7\\x9c\\x9f\\xe5\\xbf\\x83', '\\xe4\\xbd\\xa0', '\\xe8\\xae\\xa4\\xe4\\xb8\\xba', '\\xe4\\xbb\\x96\\xe4\\xbc\\x9a\\xe7\\x94\\xa8', '\\xe7\\x9c\\x9f\\xe5\\xbf\\x83', '\\xe5\\x9b\\x9e\\xe6\\x8a\\xa5', '\\xe4\\xbd\\xa0', '\\xe5\\x90\\x97', '\\xe4\\xb9\\x9f\\xe8\\xae\\xb8', '\\xe4\\xbb\\x96\\xe4\\xbc\\x9a', '\\xe4\\xbd\\x86\\xe6\\x98\\xaf', '\\xe9\\x82\\xa3', '\\xe4\\xba\\xba', '\\xe7\\xbb\\x9d\\xe5\\xaf\\xb9', '\\xe4\\xb8\\x8d\\xe6\\x98\\xaf', '\\xe4\\xbd\\xa0', '\\xe7\\xac\\xac\\xe4\\xb8\\x80', '\\xe4\\xbd\\xa0', '\\xe9\\x95\\xbf\\xe5\\xbe\\x97', '\\xe5\\xb9\\xb6', '\\xe4\\xb8\\x8d', '\\xe7\\xbe\\x8e\\xe8\\x8b\\xa5\\xe5\\xa4\\xa9\\xe4\\xbb\\x99', '\\xe7\\xac\\xac\\xe4\\xba\\x8c', '\\xe4\\xbd\\xa0', '\\xe5\\xb9\\xb6', '\\xe4\\xb8\\x8d', '\\xe5\\xae\\xb6\\xe8\\xb4\\xa2\\xe4\\xb8\\x87\\xe8\\xb4\\xaf', '\\xe5\\x8a\\x9d', '\\xe4\\xbd\\xa0', '\\xe5\\x85\\x88', '\\xe6\\x8a\\x8a', '\\xe5\\x89\\x8d', '\\xe4\\xb8\\xa4\\xe8\\x80\\x85', '\\xe6\\x8b\\xa5\\xe6\\x9c\\x89', '\\xe5\\x86\\x8d\\xe6\\x9d\\xa5', '\\xe7\\x88\\xb1', '\\xe8\\xbf\\x99\\xe6\\xa0\\xb7', '\\xe7\\x9a\\x84', '\\xe4\\xba\\xba', '\\xe5\\x90\\xa7']\n"
     ]
    }
   ],
   "source": [
    "print seg_list2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a dictionary of all the words in messages - bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100181\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "# Let's call the dictionary word_space\n",
    "from collections import defaultdict\n",
    "word_space=defaultdict(int)\n",
    "for text in seg_list2:\n",
    "    for word in text:\n",
    "        if word not in word_space:\n",
    "            word_space[word]=len(word_space) \n",
    "\n",
    "print len(word_space)\n",
    "print word_space['香港']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a sparse vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...,  0.  0.  0.]\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_sparse_vec(doc, space):\n",
    "    # create empty vector\n",
    "    sparse_vec = np.zeros(len(word_space))\n",
    "    for word in doc:\n",
    "        try:\n",
    "            sparse_vec[space[word]]=1\n",
    "        except:\n",
    "            continue\n",
    "    return sparse_vec\n",
    "\n",
    "mesg_vecs= [get_sparse_vec(mesg, word_space) for mesg in seg_list2]\n",
    "print mesg_vecs[0]\n",
    "print len(mesg_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all features to the format for SVM sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def X_construct(df): # define a function to convert features to the format for SVM sklearn\n",
    "    weibo = df['weibo_count'].tolist()\n",
    "    url = df['url_mean'].tolist()\n",
    "    mention = df['mention_mean'].tolist()\n",
    "    hashtag = df['hashtag_mean'].tolist()\n",
    "\n",
    "    #features = [weibo, url, mention, hashtag]+mesg_vecs\n",
    "    features = [weibo, url, mention, hashtag]\n",
    "    X = []\n",
    "    for i in range(len(df)):\n",
    "        obs = []\n",
    "        for f in features:\n",
    "            obs.append(f[i])\n",
    "        X.append(obs)   \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 0.0, 0.0, 0.0], [90, 0.055555555555555552, 1.4444444444444444, 0.044444444444444446]]\n"
     ]
    }
   ],
   "source": [
    "X = X_construct(df_lab0)\n",
    "print X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done fitting classifier on training data...\n",
      "\n",
      "================================================== \n",
      "\n",
      "Results with 10-fold cross validation:\n",
      "\n",
      "================================================== \n",
      "\n",
      "********************\n",
      "\t accuracy_score\t0.781428571429\n",
      "********************\n",
      "precision_score\t0.858181818182\n",
      "recall_score\t0.674285714286\n",
      "\n",
      "classification_report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.89      0.80       350\n",
      "          1       0.86      0.67      0.76       350\n",
      "\n",
      "avg / total       0.79      0.78      0.78       700\n",
      "\n",
      "\n",
      "confusion_matrix:\n",
      "\n",
      "[[311  39]\n",
      " [114 236]]\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', probability=True)\n",
    "clf.fit(X, y)\n",
    "print \"\\nDone fitting classifier on training data...\\n\"\n",
    "#------------------------------------------------------------------------------------------\n",
    "print \"=\"*50, \"\\n\"\n",
    "print \"Results with 10-fold cross validation:\\n\"\n",
    "print \"=\"*50, \"\\n\"\n",
    "#------------------------------------------------------------------------------------------\n",
    "predicted = cross_validation.cross_val_predict(clf, X, y, cv=10)\n",
    "print \"*\"*20\n",
    "print \"\\t accuracy_score\\t\", metrics.accuracy_score(y, predicted)\n",
    "print \"*\"*20\n",
    "print \"precision_score\\t\", metrics.precision_score(y, predicted)\n",
    "print \"recall_score\\t\", metrics.recall_score(y, predicted)\n",
    "print \"\\nclassification_report:\\n\\n\", metrics.classification_report(y, predicted)\n",
    "print \"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done fitting classifier on training data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', probability=True)\n",
    "clf.fit(X, y)\n",
    "print \"\\nDone fitting classifier on training data.\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use the trained classifiers to predit unlabeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already trained a classifier clf from the labeled dataset. Next we need to prepare the unlabled dataset. We extract 500 users each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select unlabled dataset (include only features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143378\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weibo_count</th>\n",
       "      <th>url_mean</th>\n",
       "      <th>mention_mean</th>\n",
       "      <th>hashtag_mean</th>\n",
       "      <th>alltext</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>u02HI5DC5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>身体是革命的本钱！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HICEX0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>爪，我要看//@ukn：  看的时候 就在想 总菊咋就让过了呢能感动你的，是因为和你相似</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HIEXCF</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>收藏。 //@uFAYJLCI： //@uATZ4IZBW：  你是帮你自己和你的孩子转。/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HIUE1R</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[得瑟]Thank you！亲[嘻嘻]【照顾宝宝最有耐心的妈妈】你排第几？ 第一：双鱼座；第...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HIUGDZ</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>合肥骆岗机场看晚霞。@uKB51ZZHY： @ukn： @ukn： @uKB55KGPV： ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           weibo_count  url_mean  mention_mean  hashtag_mean  \\\n",
       "userid                                                         \n",
       "u02HI5DC5            1       0.0      0.000000           0.0   \n",
       "u02HICEX0            2       0.0      0.500000           0.0   \n",
       "u02HIEXCF            2       0.0      2.500000           0.5   \n",
       "u02HIUE1R            9       0.0      0.222222           0.0   \n",
       "u02HIUGDZ            2       0.0      8.500000           0.0   \n",
       "\n",
       "                                                     alltext  \n",
       "userid                                                        \n",
       "u02HI5DC5                                          身体是革命的本钱！  \n",
       "u02HICEX0       爪，我要看//@ukn：  看的时候 就在想 总菊咋就让过了呢能感动你的，是因为和你相似  \n",
       "u02HIEXCF  收藏。 //@uFAYJLCI： //@uATZ4IZBW：  你是帮你自己和你的孩子转。/...  \n",
       "u02HIUE1R  [得瑟]Thank you！亲[嘻嘻]【照顾宝宝最有耐心的妈妈】你排第几？ 第一：双鱼座；第...  \n",
       "u02HIUGDZ  合肥骆岗机场看晚霞。@uKB51ZZHY： @ukn： @ukn： @uKB55KGPV： ...  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unlab = df_user.loc[(df_user['is.spammer'] == 2)]\n",
    "df_unlab = df_unlab.drop('is.spammer', 1)\n",
    "print len(df_unlab)\n",
    "df_unlab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_df_y(predP):   # define a function to construct y for the format for SVM sklearn   \n",
    "    y = []\n",
    "    p = []\n",
    "    for i in predP:\n",
    "        max_idx = []\n",
    "        if i[0] > i[1]:\n",
    "            max_idx = 0\n",
    "        elif i[0] < i[1]:\n",
    "            max_idx = 1\n",
    "        else: max_idx = ''\n",
    "        y.append(max_idx)    \n",
    "        p.append(i.max())\n",
    "    df_y = pd.DataFrame({'is.spammer': y, 'prob': p})\n",
    "    return df_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first 10,000 unlabeled data using 700 initial labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done fitting classifier on training data...\n",
      "\n",
      "================================================== \n",
      "\n",
      "Results with 10-fold cross validation:\n",
      "\n",
      "================================================== \n",
      "\n",
      "********************\n",
      "\t accuracy_score\t0.98376475931\n",
      "********************\n",
      "precision_score\t0.985110663984\n",
      "recall_score\t0.958496476116\n",
      "\n",
      "classification_report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99      6254\n",
      "          1       0.99      0.96      0.97      2554\n",
      "\n",
      "avg / total       0.98      0.98      0.98      8808\n",
      "\n",
      "\n",
      "confusion_matrix:\n",
      "\n",
      "[[6217   37]\n",
      " [ 106 2448]]\n"
     ]
    }
   ],
   "source": [
    "df_labeled = df_lab0\n",
    "for i in range(0, len(df_unlab[:10000]), 500):\n",
    "    df_trainX = df_unlab[i:i+500]     #extract 500 users each time\n",
    "    trainX = X_construct(df_trainX) \n",
    "    predY = clf.predict(trainX)\n",
    "    predP = clf.predict_proba(trainX)\n",
    "    df_predY = get_df_y(predP)\n",
    "    df_trainX = df_trainX.reset_index()\n",
    "    df_temp = pd.concat([df_trainX, df_predY], axis=1, join_axes=[df_trainX.index])\n",
    "    df_temp_hc = df_temp.loc[df_temp['prob'] >= 0.8]    #hcf means high confidence \n",
    "    df_temp_hc = df_temp_hc.set_index('userid')  #reset index to match df_labeled\n",
    "    df_labeled = df_labeled.append(df_temp_hc)\n",
    "    y = df_labeled['is.spammer'].tolist()\n",
    "    X = X_construct(df_labeled)\n",
    "    clf = svm.SVC(kernel='linear', probability=True)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "print \"\\nDone fitting classifier on training data...\\n\"\n",
    "#------------------------------------------------------------------------------------------\n",
    "print \"=\"*50, \"\\n\"\n",
    "print \"Results with 10-fold cross validation:\\n\"\n",
    "print \"=\"*50, \"\\n\"\n",
    "#------------------------------------------------------------------------------------------\n",
    "predicted = cross_validation.cross_val_predict(clf, X, y, cv=10)\n",
    "print \"*\"*20\n",
    "print \"\\t accuracy_score\\t\", metrics.accuracy_score(y, predicted)\n",
    "print \"*\"*20\n",
    "print \"precision_score\\t\", metrics.precision_score(y, predicted)\n",
    "print \"recall_score\\t\", metrics.recall_score(y, predicted)\n",
    "print \"\\nclassification_report:\\n\\n\", metrics.classification_report(y, predicted)\n",
    "print \"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done fitting classifier on training data...\n",
      "\n",
      "================================================== \n",
      "\n",
      "Results with 10-fold cross validation:\n",
      "\n",
      "================================================== \n",
      "\n",
      "********************\n",
      "\t accuracy_score\t0.983645655877\n",
      "********************\n",
      "precision_score\t0.985080645161\n",
      "recall_score\t0.958039215686\n",
      "\n",
      "classification_report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99      6255\n",
      "          1       0.99      0.96      0.97      2550\n",
      "\n",
      "avg / total       0.98      0.98      0.98      8805\n",
      "\n",
      "\n",
      "confusion_matrix:\n",
      "\n",
      "[[6218   37]\n",
      " [ 107 2443]]\n"
     ]
    }
   ],
   "source": [
    "df_labeled = df_lab0\n",
    "for i in range(0, len(df_unlab[:10000]), 500):\n",
    "    df_trainX = df_unlab[i:i+500]     #extract 500 users each time\n",
    "    trainX = X_construct(df_trainX) \n",
    "    predY = clf.predict(trainX)\n",
    "    predP = clf.predict_proba(trainX)\n",
    "    df_predY = get_df_y(predP)\n",
    "    df_trainX = df_trainX.reset_index()\n",
    "    df_temp = pd.concat([df_trainX, df_predY], axis=1, join_axes=[df_trainX.index])\n",
    "    df_temp_hc = df_temp.loc[df_temp['prob'] >= 0.8]    #hcf means high confidence \n",
    "    df_temp_hc = df_temp_hc.set_index('userid')  #reset index to match df_labeled\n",
    "    df_labeled = df_labeled.append(df_temp_hc)\n",
    "    y = df_labeled['is.spammer'].tolist()\n",
    "    X = X_construct(df_labeled)\n",
    "    clf = svm.SVC(kernel='linear', probability=True)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "print \"\\nDone fitting classifier on training data...\\n\"\n",
    "#------------------------------------------------------------------------------------------\n",
    "print \"=\"*50, \"\\n\"\n",
    "print \"Results with 10-fold cross validation:\\n\"\n",
    "print \"=\"*50, \"\\n\"\n",
    "#------------------------------------------------------------------------------------------\n",
    "predicted = cross_validation.cross_val_predict(clf, X, y, cv=10)\n",
    "print \"*\"*20\n",
    "print \"\\t accuracy_score\\t\", metrics.accuracy_score(y, predicted)\n",
    "print \"*\"*20\n",
    "print \"precision_score\\t\", metrics.precision_score(y, predicted)\n",
    "print \"recall_score\\t\", metrics.recall_score(y, predicted)\n",
    "print \"\\nclassification_report:\\n\\n\", metrics.classification_report(y, predicted)\n",
    "print \"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next 10,000 unlabeled data using 700 initial labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done fitting classifier on training data...\n",
      "\n",
      "================================================== \n",
      "\n",
      "Results with 10-fold cross validation:\n",
      "\n",
      "================================================== \n",
      "\n",
      "********************\n",
      "\t accuracy_score\t0.982945561328\n",
      "********************\n",
      "precision_score\t0.985798816568\n",
      "recall_score\t0.956006120888\n",
      "\n",
      "classification_report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99      6240\n",
      "          1       0.99      0.96      0.97      2614\n",
      "\n",
      "avg / total       0.98      0.98      0.98      8854\n",
      "\n",
      "\n",
      "confusion_matrix:\n",
      "\n",
      "[[6204   36]\n",
      " [ 115 2499]]\n"
     ]
    }
   ],
   "source": [
    "df_labeled = df_lab0\n",
    "for i in range(10000, 20000, 500):\n",
    "    df_trainX = df_unlab[i:i+500]     #extract 500 users each time\n",
    "    trainX = X_construct(df_trainX) \n",
    "    predY = clf.predict(trainX)\n",
    "    predP = clf.predict_proba(trainX)\n",
    "    df_predY = get_df_y(predP)\n",
    "    df_trainX = df_trainX.reset_index()\n",
    "    df_temp = pd.concat([df_trainX, df_predY], axis=1, join_axes=[df_trainX.index])\n",
    "    df_temp_hc = df_temp.loc[df_temp['prob'] >= 0.8]    #hcf means high confidence \n",
    "    df_temp_hc = df_temp_hc.set_index('userid')  #reset index to match df_labeled\n",
    "    df_labeled = df_labeled.append(df_temp_hc)\n",
    "    y = df_labeled['is.spammer'].tolist()\n",
    "    X = X_construct(df_labeled)\n",
    "    clf = svm.SVC(kernel='linear', probability=True)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "print \"\\nDone fitting classifier on training data...\\n\"\n",
    "#------------------------------------------------------------------------------------------\n",
    "print \"=\"*50, \"\\n\"\n",
    "print \"Results with 10-fold cross validation:\\n\"\n",
    "print \"=\"*50, \"\\n\"\n",
    "#------------------------------------------------------------------------------------------\n",
    "predicted = cross_validation.cross_val_predict(clf, X, y, cv=10)\n",
    "print \"*\"*20\n",
    "print \"\\t accuracy_score\\t\", metrics.accuracy_score(y, predicted)\n",
    "print \"*\"*20\n",
    "print \"precision_score\\t\", metrics.precision_score(y, predicted)\n",
    "print \"recall_score\\t\", metrics.recall_score(y, predicted)\n",
    "print \"\\nclassification_report:\\n\\n\", metrics.classification_report(y, predicted)\n",
    "print \"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done fitting classifier on training data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_labeled = df_lab0\n",
    "for i in range(110000, 120000, 500):\n",
    "    df_trainX = df_unlab[i:i+500]     #extract 500 users each time\n",
    "    trainX = X_construct(df_trainX) \n",
    "    predY = clf.predict(trainX)\n",
    "    predP = clf.predict_proba(trainX)\n",
    "    df_predY = get_df_y(predP)\n",
    "    df_trainX = df_trainX.reset_index()\n",
    "    df_temp = pd.concat([df_trainX, df_predY], axis=1, join_axes=[df_trainX.index])\n",
    "    df_temp_hc = df_temp.loc[df_temp['prob'] >= 0.8]    #hcf means high confidence \n",
    "    df_temp_hc = df_temp_hc.set_index('userid')  #reset index to match df_labeled\n",
    "    df_labeled = df_labeled.append(df_temp_hc)\n",
    "    y = df_labeled['is.spammer'].tolist()\n",
    "    X = X_construct(df_labeled)\n",
    "    clf = svm.SVC(kernel='linear', probability=True)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "print \"\\nDone fitting classifier on training data.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag_mean</th>\n",
       "      <th>is.spammer</th>\n",
       "      <th>mention_mean</th>\n",
       "      <th>prob</th>\n",
       "      <th>url_mean</th>\n",
       "      <th>weibo_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>u02HI3WUO</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HIC5JL</th>\n",
       "      <td>0.044444</td>\n",
       "      <td>0</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HICM0L</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.346154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HIFZWV</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u02HIGB0T</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           hashtag_mean is.spammer  mention_mean  prob  url_mean  weibo_count\n",
       "userid                                                                       \n",
       "u02HI3WUO      0.000000          0      0.000000   NaN  0.000000            6\n",
       "u02HIC5JL      0.044444          0      1.444444   NaN  0.055556           90\n",
       "u02HICM0L      0.000000          0      1.346154   NaN  0.076923           26\n",
       "u02HIFZWV      0.000000          0      2.333333   NaN  0.000000            9\n",
       "u02HIGB0T      0.000000          0      2.000000   NaN  0.000000            8"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print len(df_labeled)\n",
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2660\n"
     ]
    }
   ],
   "source": [
    "spammer1 = df_labeled.loc[df_labeled['is.spammer'] == 1] \n",
    "spammerid = spammer1.index.tolist()\n",
    "#print spammerid[:10]\n",
    "spammer_set = set(spammerid)\n",
    "print len(spammer_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6215\n"
     ]
    }
   ],
   "source": [
    "normal1 = df_labeled.loc[df_labeled['is.spammer'] == 0] \n",
    "normalid = normal1.index.tolist()\n",
    "#print normalid[:10]\n",
    "normal_set = set(normalid)\n",
    "print len(normal_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spammer_hat = []\n",
    "for user in userids:\n",
    "    if user in spammer_set:\n",
    "        spammer_hat.append(1)\n",
    "    elif user in normal_set:\n",
    "        spammer_hat.append(0)\n",
    "    else: \n",
    "        spammer_hat.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>is.spammer</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>text</th>\n",
       "      <th>url_count</th>\n",
       "      <th>userid</th>\n",
       "      <th>pred_spammer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>性感蕾丝镂空鱼嘴凉鞋，罗马风十足啊！》》》》http://t.cn/zWrRwoz</td>\n",
       "      <td>1</td>\n",
       "      <td>uWYNHUVBU</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>【美图分享】</td>\n",
       "      <td>0</td>\n",
       "      <td>uRLOCRBNC</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>佛经说：“人生有八万四千种烦恼。”小人物有小人物的烦恼，大人物有大人物的悲辛，都是不能避免。...</td>\n",
       "      <td>0</td>\n",
       "      <td>u5KY5H0QX</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>年轻没有借口</td>\n",
       "      <td>0</td>\n",
       "      <td>u5KY5H0QX</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>用自己照片作桌面会不会太……[睡觉]</td>\n",
       "      <td>0</td>\n",
       "      <td>uY02ICHJO</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hashtag_count  is.spammer  mention_count  \\\n",
       "0              0           2              0   \n",
       "1              0           2              0   \n",
       "2              0           2              1   \n",
       "3              0           2              0   \n",
       "4              0           2              0   \n",
       "\n",
       "                                                text  url_count     userid  \\\n",
       "0          性感蕾丝镂空鱼嘴凉鞋，罗马风十足啊！》》》》http://t.cn/zWrRwoz          1  uWYNHUVBU   \n",
       "1                                             【美图分享】          0  uRLOCRBNC   \n",
       "2  佛经说：“人生有八万四千种烦恼。”小人物有小人物的烦恼，大人物有大人物的悲辛，都是不能避免。...          0  u5KY5H0QX   \n",
       "3                                             年轻没有借口          0  u5KY5H0QX   \n",
       "4                                 用自己照片作桌面会不会太……[睡觉]          0  uY02ICHJO   \n",
       "\n",
       "  pred_spammer  \n",
       "0               \n",
       "1               \n",
       "2               \n",
       "3               \n",
       "4               "
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pred_spammer'] = spammer_hat\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131927\n"
     ]
    }
   ],
   "source": [
    "spammer_weibo = df[df['pred_spammer'] == 1]\n",
    "print len(spammer_weibo)\n",
    "spammer_weibo.to_csv('spammer_weibo20.csv', encoding = 'utf-8')\n",
    "#normal_weibo = df[df['pred_spammer'] == 0]\n",
    "#print len(normal_weibo)\n",
    "#normal_weibo.to_csv('normal_weibo15', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>is.spammer</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>text</th>\n",
       "      <th>url_count</th>\n",
       "      <th>userid</th>\n",
       "      <th>pred_spammer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>善用白色高光，打造立体眼妆~</td>\n",
       "      <td>0</td>\n",
       "      <td>uSIACIFSB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>蛋蛋植物</td>\n",
       "      <td>0</td>\n",
       "      <td>uSIACIFSB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>人才啊!</td>\n",
       "      <td>0</td>\n",
       "      <td>uSIACIFSB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>【夜猫交友贴】还没睡的夜猫们，出来签个到，磨叽磨叽，看看能找到同城的朋友不，[话筒] 说不定...</td>\n",
       "      <td>0</td>\n",
       "      <td>uSIACIBCZ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>早睡早起。。。。</td>\n",
       "      <td>0</td>\n",
       "      <td>uUPWTURHQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hashtag_count  is.spammer  mention_count  \\\n",
       "32              0           2              0   \n",
       "33              0           2              0   \n",
       "34              0           2              0   \n",
       "44              0           2              1   \n",
       "67              0           2              0   \n",
       "\n",
       "                                                 text  url_count     userid  \\\n",
       "32                                     善用白色高光，打造立体眼妆~          0  uSIACIFSB   \n",
       "33                                               蛋蛋植物          0  uSIACIFSB   \n",
       "34                                               人才啊!          0  uSIACIFSB   \n",
       "44  【夜猫交友贴】还没睡的夜猫们，出来签个到，磨叽磨叽，看看能找到同城的朋友不，[话筒] 说不定...          0  uSIACIBCZ   \n",
       "67                                           早睡早起。。。。          0  uUPWTURHQ   \n",
       "\n",
       "   pred_spammer  \n",
       "32            1  \n",
       "33            1  \n",
       "34            1  \n",
       "44            1  \n",
       "67            1  "
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spammer_weibo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all normal weibo messages and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal_weibo1', 'normal_weibo2', 'normal_weibo3', 'normal_weibo4', 'normal_weibo5', 'normal_weibo6', 'normal_weibo7', 'normal_weibo8', 'normal_weibo9', 'normal_weibo10', 'normal_weibo11', 'normal_weibo12', 'normal_weibo13', 'normal_weibo14', 'normal_weibo15']\n"
     ]
    }
   ],
   "source": [
    "file_list=[]\n",
    "for i in range(1,16):\n",
    "    file_name='normal_weibo'+(str)(i)\n",
    "    file_list.append(file_name)\n",
    "print file_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for filename in file_list:\n",
    "    df = pd.read_csv(filename)\n",
    "    df_list.append(df)    \n",
    "all_df = pd.concat(df_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059503\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>is.spammer</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>text</th>\n",
       "      <th>url_count</th>\n",
       "      <th>userid</th>\n",
       "      <th>pred_spammer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>傻子 过去就过去了 越想只会让自己 越来越TM看不起自己 恨自己不是吗？有什么了不起的 人不...</td>\n",
       "      <td>0</td>\n",
       "      <td>u1O52TEBM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>闲来无事，想破人QQ空间密码，于是点击了一个美女的空间，提问：“请问男生最喜欢什么花！”我暗...</td>\n",
       "      <td>0</td>\n",
       "      <td>u0AGHHPXK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Google Map真是一项伟大的发明，一个人在语言不通的地方一样可以满世界＂流浪＂，对我这...</td>\n",
       "      <td>0</td>\n",
       "      <td>u0VP213WJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>@uK3RQFVG5： 来围观一下这个@彼得潘定制手工甜品</td>\n",
       "      <td>0</td>\n",
       "      <td>u0VPP3KI2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>太可爱了[哈哈]//@uZQXIDN4T：  [哈哈]//@ukn：  谁都有二的岁月</td>\n",
       "      <td>0</td>\n",
       "      <td>u1CIX3EO2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  hashtag_count  is.spammer  mention_count  \\\n",
       "0          11              0           2              1   \n",
       "1          15              0           2              0   \n",
       "2          46              0           2              0   \n",
       "3         187              0           2              2   \n",
       "4         335              0           2              2   \n",
       "\n",
       "                                                text  url_count     userid  \\\n",
       "0  傻子 过去就过去了 越想只会让自己 越来越TM看不起自己 恨自己不是吗？有什么了不起的 人不...          0  u1O52TEBM   \n",
       "1  闲来无事，想破人QQ空间密码，于是点击了一个美女的空间，提问：“请问男生最喜欢什么花！”我暗...          0  u0AGHHPXK   \n",
       "2  Google Map真是一项伟大的发明，一个人在语言不通的地方一样可以满世界＂流浪＂，对我这...          0  u0VP213WJ   \n",
       "3                      @uK3RQFVG5： 来围观一下这个@彼得潘定制手工甜品          0  u0VPP3KI2   \n",
       "4        太可爱了[哈哈]//@uZQXIDN4T：  [哈哈]//@ukn：  谁都有二的岁月          0  u1CIX3EO2   \n",
       "\n",
       "   pred_spammer  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print len(all_df)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normal_all_df = all_df[['userid', 'text', 'hashtag_count', 'mention_count', 'url_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u1O52TEBM</td>\n",
       "      <td>傻子 过去就过去了 越想只会让自己 越来越TM看不起自己 恨自己不是吗？有什么了不起的 人不...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u0AGHHPXK</td>\n",
       "      <td>闲来无事，想破人QQ空间密码，于是点击了一个美女的空间，提问：“请问男生最喜欢什么花！”我暗...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u0VP213WJ</td>\n",
       "      <td>Google Map真是一项伟大的发明，一个人在语言不通的地方一样可以满世界＂流浪＂，对我这...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u0VPP3KI2</td>\n",
       "      <td>@uK3RQFVG5： 来围观一下这个@彼得潘定制手工甜品</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u1CIX3EO2</td>\n",
       "      <td>太可爱了[哈哈]//@uZQXIDN4T：  [哈哈]//@ukn：  谁都有二的岁月</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userid                                               text  \\\n",
       "0  u1O52TEBM  傻子 过去就过去了 越想只会让自己 越来越TM看不起自己 恨自己不是吗？有什么了不起的 人不...   \n",
       "1  u0AGHHPXK  闲来无事，想破人QQ空间密码，于是点击了一个美女的空间，提问：“请问男生最喜欢什么花！”我暗...   \n",
       "2  u0VP213WJ  Google Map真是一项伟大的发明，一个人在语言不通的地方一样可以满世界＂流浪＂，对我这...   \n",
       "3  u0VPP3KI2                      @uK3RQFVG5： 来围观一下这个@彼得潘定制手工甜品   \n",
       "4  u1CIX3EO2        太可爱了[哈哈]//@uZQXIDN4T：  [哈哈]//@ukn：  谁都有二的岁月   \n",
       "\n",
       "   hashtag_count  mention_count  url_count  \n",
       "0              0              1          0  \n",
       "1              0              0          0  \n",
       "2              0              0          0  \n",
       "3              0              2          0  \n",
       "4              0              2          0  "
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_all_df.to_csv('normal_all', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the saved normal messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u1O52TEBM</td>\n",
       "      <td>傻子 过去就过去了 越想只会让自己 越来越TM看不起自己 恨自己不是吗？有什么了不起的 人不...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u0AGHHPXK</td>\n",
       "      <td>闲来无事，想破人QQ空间密码，于是点击了一个美女的空间，提问：“请问男生最喜欢什么花！”我暗...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u0VP213WJ</td>\n",
       "      <td>Google Map真是一项伟大的发明，一个人在语言不通的地方一样可以满世界＂流浪＂，对我这...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u0VPP3KI2</td>\n",
       "      <td>@uK3RQFVG5： 来围观一下这个@彼得潘定制手工甜品</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u1CIX3EO2</td>\n",
       "      <td>太可爱了[哈哈]//@uZQXIDN4T：  [哈哈]//@ukn：  谁都有二的岁月</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userid                                               text  \\\n",
       "0  u1O52TEBM  傻子 过去就过去了 越想只会让自己 越来越TM看不起自己 恨自己不是吗？有什么了不起的 人不...   \n",
       "1  u0AGHHPXK  闲来无事，想破人QQ空间密码，于是点击了一个美女的空间，提问：“请问男生最喜欢什么花！”我暗...   \n",
       "2  u0VP213WJ  Google Map真是一项伟大的发明，一个人在语言不通的地方一样可以满世界＂流浪＂，对我这...   \n",
       "3  u0VPP3KI2                      @uK3RQFVG5： 来围观一下这个@彼得潘定制手工甜品   \n",
       "4  u1CIX3EO2        太可爱了[哈哈]//@uZQXIDN4T：  [哈哈]//@ukn：  谁都有二的岁月   \n",
       "\n",
       "   hashtag_count  mention_count  url_count  \n",
       "0              0              1          0  \n",
       "1              0              0          0  \n",
       "2              0              0          0  \n",
       "3              0              2          0  \n",
       "4              0              2          0  "
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('normal_all')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all spams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spammer_weibo1', 'spammer_weibo2', 'spammer_weibo3', 'spammer_weibo4', 'spammer_weibo5', 'spammer_weibo6', 'spammer_weibo7', 'spammer_weibo8', 'spammer_weibo9', 'spammer_weibo10', 'spammer_weibo11']\n"
     ]
    }
   ],
   "source": [
    "file_list=[]\n",
    "for i in range(1,16):\n",
    "    file_name='spammer_weibo'+(str)(i)\n",
    "    file_list.append(file_name)\n",
    "print file_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#too large to merge into one df at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for filename in file_list:\n",
    "    df0 = pd.read_csv(filename)\n",
    "    df_list.append(df0)    \n",
    "all_df = pd.concat(df_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#spammer_all_df = pd.concat([all_df, all_df2, spammer_weibo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562107\n",
      "363872\n",
      "131927\n",
      "2057906\n"
     ]
    }
   ],
   "source": [
    "print len(all_df)\n",
    "print len(all_df2)\n",
    "print len(spammer_weibo)\n",
    "print len(spammer_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#spammer_all_df.to_csv('spammer_all', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1_user = pd.DataFrame()\n",
    "df1_user['weibo_count'] = spammer_all_df.groupby(['userid']).size()\n",
    "df1_user['url_mean'] = spammer_all_df.groupby(['userid'])['url_count'].mean()\n",
    "df1_user['mention_mean'] = spammer_all_df.groupby(['userid'])['mention_count'].mean()\n",
    "df1_user['hashtag_mean'] = spammer_all_df.groupby(['userid'])['hashtag_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32513\n",
      "           weibo_count  url_mean  mention_mean  hashtag_mean\n",
      "userid                                                      \n",
      "u02HIMXNX          285  0.105263      1.947368      0.000000\n",
      "u02HIWMHH           71  0.422535      0.380282      0.014085\n",
      "u02HR20GP            2  1.000000      0.000000      0.000000\n",
      "u02HRIHXV         1185  0.278481      0.518987      0.620253\n",
      "u02HRLGB1          270  0.277778      0.000000      0.000000\n",
      "      userid  weibo_count  url_mean  mention_mean  hashtag_mean\n",
      "0  u02HIMXNX          285  0.105263      1.947368      0.000000\n",
      "1  u02HIWMHH           71  0.422535      0.380282      0.014085\n",
      "2  u02HR20GP            2  1.000000      0.000000      0.000000\n",
      "3  u02HRIHXV         1185  0.278481      0.518987      0.620253\n",
      "4  u02HRLGB1          270  0.277778      0.000000      0.000000\n"
     ]
    }
   ],
   "source": [
    "print len(df1_user)\n",
    "print df1_user.head()\n",
    "df1_user = df1_user.reset_index()\n",
    "print df1_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uQZ0ZT0QG', 'uVEQW3AIG', 'uVGJLOI2R', 'uYC335VVO']"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = open('oversea_users.csv')\n",
    "csv_f1 = csv.reader(f1)\n",
    "\n",
    "oversea = []\n",
    "for row in csv_f1:\n",
    "    oversea.append(row[0])\n",
    "    \n",
    "oversea[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26010\n"
     ]
    }
   ],
   "source": [
    "oversea_user_set = set(oversea)\n",
    "print len(oversea_user_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32513\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "is_oversea = []\n",
    "for index, row in df1_user.iterrows():\n",
    "    if row['userid'] in oversea_user_set:\n",
    "        is_oversea.append(1)\n",
    "    else:\n",
    "        is_oversea.append(0)\n",
    "print len(is_oversea)\n",
    "print is_oversea[:10]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>weibo_count</th>\n",
       "      <th>url_mean</th>\n",
       "      <th>mention_mean</th>\n",
       "      <th>hashtag_mean</th>\n",
       "      <th>is_oversea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u02HIMXNX</td>\n",
       "      <td>285</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>1.947368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u02HIWMHH</td>\n",
       "      <td>71</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.380282</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u02HR20GP</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u02HRIHXV</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.278481</td>\n",
       "      <td>0.518987</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u02HRLGB1</td>\n",
       "      <td>270</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userid  weibo_count  url_mean  mention_mean  hashtag_mean  is_oversea\n",
       "0  u02HIMXNX          285  0.105263      1.947368      0.000000           0\n",
       "1  u02HIWMHH           71  0.422535      0.380282      0.014085           0\n",
       "2  u02HR20GP            2  1.000000      0.000000      0.000000           0\n",
       "3  u02HRIHXV         1185  0.278481      0.518987      0.620253           0\n",
       "4  u02HRLGB1          270  0.277778      0.000000      0.000000           0"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_user['is_oversea'] = is_oversea\n",
    "df1_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of oversea users in this weekly weibo data is 15447\n",
      "The number of oversea users among spammers is 2951\n"
     ]
    }
   ],
   "source": [
    "oversea_df1 = df1_user[df1_user['is_oversea'] == 1]\n",
    "print 'Total number of oversea users in this weekly weibo data is 15447'\n",
    "print 'The number of oversea users among spammers is', len(oversea_df1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>is.spammer</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>text</th>\n",
       "      <th>url_count</th>\n",
       "      <th>userid</th>\n",
       "      <th>pred_spammer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>善用白色高光，打造立体眼妆~</td>\n",
       "      <td>0</td>\n",
       "      <td>uSIACIFSB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>蛋蛋植物</td>\n",
       "      <td>0</td>\n",
       "      <td>uSIACIFSB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>人才啊!</td>\n",
       "      <td>0</td>\n",
       "      <td>uSIACIFSB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>【夜猫交友贴】还没睡的夜猫们，出来签个到，磨叽磨叽，看看能找到同城的朋友不，[话筒] 说不定...</td>\n",
       "      <td>0</td>\n",
       "      <td>uSIACIBCZ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>早睡早起。。。。</td>\n",
       "      <td>0</td>\n",
       "      <td>uUPWTURHQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hashtag_count  is.spammer  mention_count  \\\n",
       "32              0           2              0   \n",
       "33              0           2              0   \n",
       "34              0           2              0   \n",
       "44              0           2              1   \n",
       "67              0           2              0   \n",
       "\n",
       "                                                 text  url_count     userid  \\\n",
       "32                                     善用白色高光，打造立体眼妆~          0  uSIACIFSB   \n",
       "33                                               蛋蛋植物          0  uSIACIFSB   \n",
       "34                                               人才啊!          0  uSIACIFSB   \n",
       "44  【夜猫交友贴】还没睡的夜猫们，出来签个到，磨叽磨叽，看看能找到同城的朋友不，[话筒] 说不定...          0  uSIACIBCZ   \n",
       "67                                           早睡早起。。。。          0  uUPWTURHQ   \n",
       "\n",
       "   pred_spammer  \n",
       "32            1  \n",
       "33            1  \n",
       "34            1  \n",
       "44            1  \n",
       "67            1  "
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spammer_weibo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1_user = pd.DataFrame()\n",
    "df1_user['weibo_count'] = spammer_all_df.groupby(['userid']).size()\n",
    "df1_user['url_mean'] = spammer_all_df.groupby(['userid'])['url_count'].mean()\n",
    "df1_user['mention_mean'] = spammer_all_df.groupby(['userid'])['mention_count'].mean()\n",
    "df1_user['hashtag_mean'] = spammer_all_df.groupby(['userid'])['hashtag_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_oversea = []\n",
    "for index, row in df1_user.iterrows():\n",
    "    if row['userid'] in oversea_user_set:\n",
    "        is_oversea.append(1)\n",
    "    else:\n",
    "        is_oversea.append(0)\n",
    "print len(is_oversea)\n",
    "print is_oversea[:10]  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
